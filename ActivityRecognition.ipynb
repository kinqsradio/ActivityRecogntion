{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "-SDEq7nOPb5Z",
   "metadata": {
    "id": "-SDEq7nOPb5Z"
   },
   "source": [
    "# Introduction\n",
    "This repository contains the codebase for an advanced video analysis pipeline that combines the power of multiple state-of-the-art models and methodologies including YOLO V8, ByteTrack, Movenet, and Transformer encoders for high-accuracy activity recognition. The pipeline is trained on the Human Activity Recognition (HAR - Video Dataset)\n",
    "\n",
    "Reference:\n",
    "Datasets: https://www.kaggle.com/datasets/sharjeelmazhar/human-activity-recognition-video-dataset?resource=download-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2b639",
   "metadata": {
    "id": "afa2b639"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i0BxdQaQxVN",
   "metadata": {
    "id": "4i0BxdQaQxVN"
   },
   "source": [
    "\n",
    "\n",
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59c645",
   "metadata": {
    "id": "7b59c645"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package_name, pip_command=None):\n",
    "    try:\n",
    "        # Check if the package is installed by trying to import it\n",
    "        exec(f\"import {package_name}\")\n",
    "    except ImportError:\n",
    "        # If the package is not installed, use pip to install it\n",
    "        if not pip_command:\n",
    "            pip_command = package_name\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_command])\n",
    "\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Usage\n",
    "packages_to_install = {\n",
    "    'cv2': 'opencv-python-headless',\n",
    "    'tensorflow': None,\n",
    "    'tensorflow': 'tensorflow-gpu==2.4.1',\n",
    "    'cv2': 'opencv-python',\n",
    "    'matplotlib': None,\n",
    "    'imageio': None,\n",
    "    'tfdocs': 'git+https://github.com/tensorflow/docs',\n",
    "    'transformers': 'git+https://github.com/huggingface/transformers',\n",
    "    'sklearn': 'scikit-learn',\n",
    "    'scipy': None,\n",
    "    'matplotlib': None,\n",
    "    'tensorflow_hub': None\n",
    "}\n",
    "\n",
    "for package, command in packages_to_install.items():\n",
    "    install_package(package, command)\n",
    "\n",
    "!pip install timm\n",
    "\n",
    "%cd \"C:/Users/Duc Anh/Desktop/#ActivityRecognition/packages/ultralytics\"\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emHAgLMfQ7UN",
   "metadata": {
    "id": "emHAgLMfQ7UN"
   },
   "source": [
    "#  Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c57526",
   "metadata": {
    "id": "80c57526"
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "# Third-party Libraries\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "# Custom Module Imports\n",
    "from train import initialize_dense_model, initialize_lstm_model, initialize_cnn_model, train_model\n",
    "from datasets import create_datasets, create_cnn_datasets\n",
    "\n",
    "# Constants or Configuration\n",
    "from torchvision.models.resnet import ResNet152_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ece336",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 27\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)\n",
    "\n",
    "SEQUENCE_LENGTH = 50\n",
    "\n",
    "# Datasets\n",
    "DATASET_DIR = 'C:/Users/Duc Anh/Desktop/#ActivityRecognition/datasets/Human Activity Recognition - Video Dataset'\n",
    "CLASSES_LIST = sorted([entry.name for entry in os.scandir(DATASET_DIR) if entry.is_dir()])\n",
    "print(f'{CLASSES_LIST}')\n",
    "print(f'LENGTH: {len(CLASSES_LIST)}')\n",
    "\n",
    "# Setting Up Saving Folder\n",
    "final = \"C:/Users/Duc Anh/Desktop/#ActivityRecognition/final\"\n",
    "if not os.path.exists(final):\n",
    "    os.makedirs(final)\n",
    "%cd \"C:/Users/Duc Anh/Desktop/#ActivityRecognition/final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y2WhIoBhQ9cl",
   "metadata": {
    "id": "Y2WhIoBhQ9cl"
   },
   "source": [
    "# Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c79597",
   "metadata": {
    "id": "24c79597"
   },
   "outputs": [],
   "source": [
    "# Initialize YOLO model\n",
    "yolo_model = YOLO('yolov8x.pt')\n",
    "\n",
    "# For spatial features\n",
    "spatial_resnet = models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)\n",
    "spatial_resnet = nn.Sequential(*list(spatial_resnet.children())[:-1])\n",
    "spatial_resnet.eval()\n",
    "\n",
    "# For temporal features\n",
    "temporal_resnet = models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)\n",
    "temporal_resnet = nn.Sequential(*list(temporal_resnet.children())[:-1])\n",
    "temporal_resnet.eval()\n",
    "\n",
    "\n",
    "# Define Transformer Encoders\n",
    "# fewer attention heads for spatial model\n",
    "spatial_encoder_layers = TransformerEncoderLayer(d_model=2048, nhead=4)  # fewer attention heads for spatial model\n",
    "spatial_transformer = TransformerEncoder(spatial_encoder_layers, num_layers=2)\n",
    "\n",
    "# more attention heads for temporal model\n",
    "temporal_encoder_layers = TransformerEncoderLayer(d_model=2048, nhead=16)  # more attention heads for temporal model\n",
    "temporal_transformer = TransformerEncoder(temporal_encoder_layers, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rNxx4tIMTCX_",
   "metadata": {
    "id": "rNxx4tIMTCX_"
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e17092",
   "metadata": {
    "id": "40e17092"
   },
   "outputs": [],
   "source": [
    "# Datasets for Dense + LSTM\n",
    "%cd \"C:/Users/Duc Anh/Desktop/#ActivityRecognition/final/create_datasets\"\n",
    "# Preparing Datasets for Training\n",
    "dl_features, dl_labels, dl_video_files_paths = create_datasets(yolo_model,\n",
    "                                                      spatial_resnet,\n",
    "                                                      spatial_transformer,\n",
    "                                                      temporal_resnet,\n",
    "                                                      temporal_transformer,\n",
    "                                                      DATASET_DIR,\n",
    "                                                      CLASSES_LIST,\n",
    "                                                      SEQUENCE_LENGTH,\n",
    "                                                      NUM_VIDEOS_TO_PROCESS=25)\n",
    "\n",
    "# Drop any NaN\n",
    "if np.isnan(dl_features).any():\n",
    "  # Identify sequences that contain any nan values\n",
    "  sequences_with_nan = np.any(np.isnan(dl_features), axis=(1, 2))\n",
    "\n",
    "  # Filter out those sequences\n",
    "  dl_features = dl_features[~sequences_with_nan]\n",
    "  dl_labels = dl_labels[~sequences_with_nan]\n",
    "\n",
    "  print(f\"Original number of sequences: {dl_features.shape[0]}\")\n",
    "  print(f\"Number of sequences after removal: {dl_labels.shape[0]}\")\n",
    "  print(f'{dl_features.shape}')\n",
    "  print(f'{dl_labels.shape}')\n",
    "\n",
    "# Data Augmented, this might be a good testing to scale the datasets for training\n",
    "from datasets import augment_features\n",
    "\n",
    "# # Generate the augmented data\n",
    "all_augmented_features = augment_features(dl_features)\n",
    "\n",
    "# # Combine original and all augmented versions\n",
    "all_features = np.concatenate((dl_features, all_augmented_features), axis=0)\n",
    "\n",
    "# # Create labels for the augmented data\n",
    "all_labels = np.tile(dl_labels, (all_features.shape[0] // dl_features.shape[0],))\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "dl_one_hot_encoded_labels = to_categorical(all_labels)\n",
    "dl_features_train, dl_features_test, dl_labels_train, dl_labels_test = train_test_split(all_features, dl_one_hot_encoded_labels,\n",
    "                                                                            test_size = 0.2, shuffle = True,\n",
    "                                                                            random_state = seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets for CNN Model\n",
    "%cd \"C:/Users/Duc Anh/Desktop/#ActivityRecognition/final/create_cnn_datasets\"\n",
    "cnn_features, cnn_labels, cnn_video_files_paths = create_cnn_datasets(DATASET_DIR,\n",
    "                                                         CLASSES_LIST,\n",
    "                                                         SEQUENCE_LENGTH,\n",
    "                                                         NUM_VIDEOS_TO_PROCESS=25)\n",
    "\n",
    "# Drop any NaN\n",
    "if np.isnan(cnn_features).any():\n",
    "  # Identify sequences that contain any nan values\n",
    "  sequences_with_nan = np.any(np.isnan(cnn_features), axis=(1, 2))\n",
    "\n",
    "  # Filter out those sequences\n",
    "  cnn_features = features[~sequences_with_nan]\n",
    "  cnn_labels = cnn_labels[~sequences_with_nan]\n",
    "\n",
    "  print(f\"Original number of sequences: {cnn_features.shape[0]}\")\n",
    "  print(f\"Number of sequences after removal: {cnn_labels.shape[0]}\")\n",
    "  print(f'{cnn_features.shape}')\n",
    "  print(f'{cnn_labels.shape}')\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "cnn_one_hot_encoded_labels = to_categorical(cnn_labels)\n",
    "# Please run this if you want to split the data without Augment!\n",
    "cnn_features_train, cnn_features_test, cnn_labels_train, cnn_labels_test = train_test_split(cnn_features, cnn_one_hot_encoded_labels,\n",
    "                                                                            test_size = 0.2, shuffle = True,\n",
    "                                                                            random_state = seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dense + LSTM')\n",
    "print(f'Features Train: {dl_features_train.shape}')\n",
    "print(f'Labels Train: {dl_labels_train.shape}')\n",
    "print(f'Features Test: {dl_features_test.shape}')\n",
    "print(f'Labels Test: {dl_labels_test.shape}\\n')\n",
    "print('CNN')\n",
    "print(f'Features Train: {cnn_features_train.shape}')\n",
    "print(f'Labels Train: {cnn_labels_train.shape}')\n",
    "print(f'Features Test: {cnn_features_test.shape}')\n",
    "print(f'Labels Test: {cnn_labels_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tY_Moa3GTsz2",
   "metadata": {
    "id": "tY_Moa3GTsz2"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"C:/Users/Duc Anh/Desktop/#ActivityRecognition/final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d71e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (dl_features.shape[1], dl_features.shape[2])\n",
    "num_classes = len(CLASSES_LIST)\n",
    "\n",
    "# Dense Model\n",
    "dense_model = initialize_dense_model(input_shape, num_classes)\n",
    "dense_model.summary()\n",
    "\n",
    "# Train model\n",
    "dense_model_name = 'dense_model'\n",
    "dense_model = train_model(dense_model_name, \n",
    "                          dense_model, \n",
    "                          dl_features_train, \n",
    "                          dl_labels_train, \n",
    "                          batch_size=1, \n",
    "                          epochs=1000000000000, \n",
    "                          early_stopping_patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a630d4",
   "metadata": {
    "id": "e1a630d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (dl_features.shape[1], dl_features.shape[2])\n",
    "num_classes = len(CLASSES_LIST)\n",
    "\n",
    "# LSTM MODEL\n",
    "lstm_model = initialize_lstm_model(input_shape, num_classes)\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train model\n",
    "lstm_model_name = 'lstm_model'\n",
    "lstm_model = train_model(lstm_model_name, \n",
    "                         lstm_model, \n",
    "                         dl_features_train, \n",
    "                         dl_labels_train, \n",
    "                         batch_size=1, \n",
    "                         epochs=1000000000000, \n",
    "                         early_stopping_patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16160b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (cnn_features.shape[1], cnn_features.shape[2], cnn_features.shape[3], cnn_features.shape[4])\n",
    "num_classes = len(CLASSES_LIST)\n",
    "\n",
    "# CNN Model\n",
    "cnn_model = initialize_cnn_model(input_shape, num_classes)\n",
    "cnn_model.summary()\n",
    "\n",
    "\n",
    "# Train model\n",
    "cnn_model_name = 'cnn_model'\n",
    "cnn_model = train_model(cnn_model_name, \n",
    "                         cnn_model, \n",
    "                         cnn_features_train, \n",
    "                         cnn_labels_train, \n",
    "                         batch_size=4, \n",
    "                         epochs=1000000000000, \n",
    "                         early_stopping_patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GxiQ-gBcT8Fv",
   "metadata": {
    "id": "GxiQ-gBcT8Fv"
   },
   "source": [
    "# Predict on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"C:/Users/Duc Anh/Desktop/#ActivityRecognition/final/\"\n",
    "\n",
    "# Load the state dict previously saved \n",
    "dense_model_name = 'dense_model.h5'\n",
    "dense_model = keras.models.load_model(dense_model_name)\n",
    "\n",
    "lstm_model_name = 'lstm_model.h5'\n",
    "lstm_model = keras.models.load_model(lstm_model_name)\n",
    "\n",
    "cnn_model_name = 'cnn_model.h5'\n",
    "cnn_model = keras.models.load_model(cnn_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d0e54",
   "metadata": {
    "id": "4d2d0e54"
   },
   "outputs": [],
   "source": [
    "from predict import predict_on_video\n",
    "video_path = 'C:/Users/Duc Anh/Desktop/ActivityRecognition/test/videos/#TEST2.mp4'\n",
    "output_video = '#TEST2.mp4'\n",
    "predict_on_video(video_path,\n",
    "                 output_video,\n",
    "                 CLASSES_LIST,\n",
    "                 dense_model,\n",
    "                 lstm_model,\n",
    "                 cnn_model,\n",
    "                 yolo_model,\n",
    "                 spatial_resnet,\n",
    "                 spatial_transformer,\n",
    "                 temporal_resnet, \n",
    "                 temporal_transformer,\n",
    "                 SEQUENCE_LENGTH=50,\n",
    "                 debug=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
